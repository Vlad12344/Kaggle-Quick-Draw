{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle_RAM_dataset.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vlad12344/Inverted-Pendulum-Project/blob/master/Kaggle_RAM_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "MMq6SYV5sALh",
        "colab_type": "code",
        "outputId": "47e1278c-5c4d-404c-e7e7-85964f736f43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing import image\n",
        "from PIL import Image, ImageDraw, ImageOps\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Activation, Dropout\n",
        "from keras.optimizers import Adadelta\n",
        "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
        "from keras import regularizers\n",
        "from keras.applications import MobileNet, VGG16\n",
        "from keras.models import model_from_json\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# seed для повторяемости результатов\n",
        "np.random.seed(42)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oV_aMTLAnwdJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "size = 32\n",
        "#PATH = '/content/drive/My Drive/colab/dataset_32x32/random_dat'\n",
        "PATH = '/content/drive/My Drive/colab/dataset_50'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ITZ3PynksALo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = np.load(os.path.join(PATH, 'X_train.npy'))\n",
        "Y_train = np.load(os.path.join(PATH, 'Y_train.npy'))\n",
        "x_test = np.load(os.path.join(PATH, 'x_test.npy'))\n",
        "y_test = np.load(os.path.join(PATH, 'y_test.npy'))\n",
        "\n",
        "x_test = x_test.astype('float16')\n",
        "X_train = X_train.astype('float16')\n",
        "\n",
        "X_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "Y_train = np_utils.to_categorical(Y_train)\n",
        "\n",
        "#y_test = np.delete(y_test, np.s_[0:0], 1)\n",
        "#Y_train = np.delete(Y_train, np.s_[0:0], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "99OGgjxLsALy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Convolution2D(filters = 64, kernel_size = (3, 3), padding = 'same',\n",
        "                 input_shape = (size,size,1), activation = 'relu', \n",
        "                 data_format = \"channels_last\"))\n",
        "#model.add(BatchNormalization())\n",
        "\n",
        "model.add(Convolution2D(filters = 64, kernel_size = (3, 3), padding = 'valid',\n",
        "                 activation = 'relu', data_format = \"channels_last\"))\n",
        "#model.add(BatchNormalization())\n",
        "\n",
        "\n",
        "model.add(MaxPooling2D(pool_size = (2,2), \n",
        "                       data_format = \"channels_last\"))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Convolution2D(filters = 128, kernel_size = (3, 3), padding = 'valid',\n",
        "                       activation = 'relu', data_format = \"channels_last\"))\n",
        "#model.add(BatchNormalization())\n",
        "          \n",
        "model.add(Convolution2D(filters = 128, kernel_size = (3, 3), padding = 'valid',\n",
        "                       activation = 'relu', data_format = \"channels_last\"))\n",
        "#model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size = (2,2), \n",
        "                       data_format = \"channels_last\"))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(120, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(200, activation = 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(50, activation = 'softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5sgHkzy2sAL1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eR-uKTlesAL5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optim = Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optim, \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aR8E3X5DsAL9",
        "colab_type": "code",
        "outputId": "498027a6-bb3d-47ff-ccdf-267896040513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "for i in range(1):\n",
        "  \n",
        "  history = model.fit(X_train, Y_train, epochs=1, validation_split=0.15, shuffle=True, batch_size=500)\n",
        "\n",
        "  model.save_weights(os.path.join(PATH, 'quick_doodle.h5'))\n",
        "\n",
        "  model_json = model.to_json()\n",
        "# Записываем модель в файл\n",
        "  json_file = open(os.path.join(PATH, \"quick_doodle.json\"), 'w')\n",
        "  json_file.write(model_json)\n",
        "  json_file.close()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 255000 samples, validate on 45000 samples\n",
            "Epoch 1/1\n",
            "255000/255000 [==============================] - 80s 312us/step - loss: 3.0840 - acc: 0.1953 - val_loss: 1.9355 - val_acc: 0.4878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Pmqi1rZrsAMC",
        "colab_type": "code",
        "outputId": "d33e54c3-f983-4566-9bd9-756d221b0f12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "scores = model.predict(x_test, batch_size = 128, verbose=1)\n",
        "print(np.max(scores[0,:]))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 6s 111us/step\n",
            "0.46782935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uXyQ6uZ_sAMH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "model_json = model.to_json()\n",
        "\n",
        "json_file = open('/media/vlados/FreeSpace/Kaggle/dataset/quick_doodle.h5', \"w\")\n",
        "json_file.write(model_json)\n",
        "json_file.close()"
      ]
    },
    {
      "metadata": {
        "id": "cYhEF6lqsAMI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save_weights(os.path.join(PATH, 'quick_doodle.h5'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kvAgrmfgsAMM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#json_file = open(\"/content/drive/My Drive/colab/dataset_32x32/quick_doodle_M.json\", \"r\")\n",
        "#loaded_model_json = json_file.read()\n",
        "#json_file.close()\n",
        "# Создаем модель на основе загруженных данных\n",
        "#loaded_model = model_from_json(loaded_model_json)\n",
        "# Загружаем веса в модель\n",
        "loaded_model.load_weights('/content/drive/My Drive/colab/dataset_32x32/quick_doodle.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h5Bw-rJpsAMP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loaded_model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
        "\n",
        "loaded_model.fit(X_train, Y_train, epochs=5, validation_split=0.2, shuffle=True, batch_size=128)\n",
        "\n",
        "scores = loaded_model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Точность модели на тестовых данных: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HNXoPJ0isAMY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model_json = model.to_json()\n",
        "# Записываем модель в файл\n",
        "json_file = open(\"/content/drive/My Drive/colab/dataset_32x32/random_dat/quick_doodle.json\", \"w\")\n",
        "json_file.write(model_json)\n",
        "json_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "INSnwTG__d6O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#classes = np.load(os.path.join(PATH,'classes.npy'))\n",
        "optim = Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
        "\n",
        "model = MobileNet(input_shape=(size, size, 1), alpha=1, weights=None, classes=50)\n",
        "model.compile(optimizer=optim, loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X-k726mPARJO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  \n",
        "  history = model.fit(X_train, Y_train, epochs=1, validation_split=0.15, shuffle=True, batch_size=128) \n",
        "  model.save_weights(os.path.join(PATH, 'quick_doodle_M.h5'))\n",
        "\n",
        "  model_json = model.to_json()\n",
        "# Записываем модель в файл\n",
        "  json_file = open(os.path.join(PATH, \"quick_doodle_M.json\"), 'w')\n",
        "  json_file.write(model_json)\n",
        "  json_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2mm9DHLaLwHr",
        "colab_type": "code",
        "outputId": "3e1778ac-0d0c-4779-e20c-fe6a032cf9d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Точность модели на тестовых данных: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Точность модели на тестовых данных: 1.24%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Uqz5llzLJjB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_test = np.load('/content/drive/My Drive/colab/dataset_32x32/random_dat/x__test.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "euaFegANLZhl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "Input_dir = '/content/drive/My Drive/colab/dataset/' #csv files path\n",
        "test_predictions = model.predict(x_test, batch_size=128, verbose=1)\n",
        "\n",
        "def top3(predictions):\n",
        "    \n",
        "    classes = np.load('/content/drive/My Drive/colab/dataset_32x32/random_dat/classes.npy')\n",
        "    top_3 =  np.argsort(-predictions)[:, 0:3]\n",
        "    top_3_name = np.empty(top_3.shape, dtype=object)\n",
        "    \n",
        "    for row in range(len(top_3)):\n",
        "        for element in range(3):\n",
        "            top_3_name[row,element] = re.sub(' ', '_', classes[top_3[row,element]]) \n",
        "    \n",
        "    return(top_3_name)\n",
        "  \n",
        "top = top3(test_predictions)\n",
        "\n",
        "preds_df = pd.DataFrame({'first': top[:,0], 'second': top[:,1], 'third': top[:,2]})\n",
        "preds_df['words'] = preds_df['first'] + \" \" + preds_df['second'] + \" \" + preds_df['third']\n",
        "\n",
        "sub = pd.read_csv(Input_dir + 'sample_submission.csv', index_col=['key_id'])\n",
        "sub['word'] = preds_df.words.values\n",
        "sub.to_csv('/content/drive/My Drive/colab/dataset_32x32/1class_per_label_proto_M.csv')\n",
        "sub.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_ylZyYersj5-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_import(submit_file):\n",
        "  json_file = open(os.path.join(PATH, '50_1', 'quick_doodle.json'), \"r\")\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "# Создаем модель на основе загруженных данных\n",
        "  loaded_model_1 = model_from_json(loaded_model_json)\n",
        "# Загружаем веса в модель\n",
        "  loaded_model_1.load_weights(os.path.join(PATH, '50_1', 'quick_doodle.h5'))\n",
        "  \n",
        "  json_file = open(os.path.join(PATH, '50_2', 'quick_doodle.json'), \"r\")\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "# Создаем модель на основе загруженных данных\n",
        "  loaded_model_2 = model_from_json(loaded_model_json)\n",
        "# Загружаем веса в модель\n",
        "  loaded_model_2.load_weights(os.path.join(PATH, '50_2', 'quick_doodle.h5'))\n",
        "  \n",
        "  json_file = open(os.path.join(PATH, '50_3', 'quick_doodle.json'), \"r\")\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "# Создаем модель на основе загруженных данных\n",
        "  loaded_model_3 = model_from_json(loaded_model_json)\n",
        "# Загружаем веса в модель\n",
        "  loaded_model_3.load_weights(os.path.join(PATH, '50_3', 'quick_doodle.h5'))\n",
        "  \n",
        "  json_file = open(os.path.join(PATH, '50_4', 'quick_doodle.json'), \"r\")\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "# Создаем модель на основе загруженных данных\n",
        "  loaded_model_4 = model_from_json(loaded_model_json)\n",
        "# Загружаем веса в модель\n",
        "  loaded_model_4.load_weights(os.path.join(PATH, '50_4', 'quick_doodle.h5'))\n",
        "  \n",
        "  json_file = open(os.path.join(PATH, '50_5', 'quick_doodle.json'), \"r\")\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "# Создаем модель на основе загруженных данных\n",
        "  loaded_model_5 = model_from_json(loaded_model_json)\n",
        "# Загружаем веса в модель\n",
        "  loaded_model_5.load_weights(os.path.join(PATH, '50_5', 'quick_doodle.h5'))\n",
        "  \n",
        "  json_file = open(os.path.join(PATH, '50_6', 'quick_doodle.json'), \"r\")\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "# Создаем модель на основе загруженных данных\n",
        "  loaded_model_6 = model_from_json(loaded_model_json)\n",
        "# Загружаем веса в модель\n",
        "  loaded_model_6.load_weights(os.path.join(PATH, '50_6', 'quick_doodle.h5'))\n",
        "  \n",
        "  json_file = open(os.path.join(PATH, '50_7', 'quick_doodle.json'), \"r\")\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "# Создаем модель на основе загруженных данных\n",
        "  loaded_model_7 = model_from_json(loaded_model_json)\n",
        "# Загружаем веса в модель\n",
        "  loaded_model_7.load_weights(os.path.join(PATH, '50_7', 'quick_doodle.h5'))\n",
        "  \n",
        "  pred_1 = loaded_model_1.predict(submit_file, batch_size=128, verbose=1)\n",
        "  pred_2 = loaded_model_2.predict(submit_file, batch_size=128, verbose=1)\n",
        "  pred_3 = loaded_model_3.predict(submit_file, batch_size=128, verbose=1)\n",
        "  pred_4 = loaded_model_4.predict(submit_file, batch_size=128, verbose=1)\n",
        "  pred_5 = loaded_model_5.predict(submit_file, batch_size=128, verbose=1)\n",
        "  pred_6 = loaded_model_6.predict(submit_file, batch_size=128, verbose=1)\n",
        "  pred_7 = loaded_model_7.predict(submit_file, batch_size=128, verbose=1)\n",
        "  \n",
        "  total_pred = np.concatenate((pred_1, pred_2, pred_3, \n",
        "                              pred_4,pred_5, pred_6, pred_7), axis=1, )\n",
        "  return(total_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uQyvZDdjswGP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "f0e34910-db9f-4660-aa80-f396ea4c1bac"
      },
      "cell_type": "code",
      "source": [
        "x_test = np.load(os.path.join(PATH, 'x__test.npy'))\n",
        "\n",
        "model = MobileNet(input_shape=(size, size, 1), alpha=1, weights=os.path.join(PATH, 'quick_doodle_M.h5'), classes=340)\n",
        "predict_mobile = model.predict(x_test, batch_size=128, verbose=1)\n",
        "\n",
        "json_file = open(os.path.join(PATH, 'quick_doodle.json'), \"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "\n",
        "# Создаем модель на основе загруженных данных\n",
        "loaded_model = model_from_json(loaded_model_json)\n",
        "\n",
        "# Загружаем веса в модель\n",
        "loaded_model.load_weights(os.path.join(PATH, 'quick_doodle.h5'))\n",
        "\n",
        "predict_standart = loaded_model.predict(x_test, batch_size=128, verbose=1)\n",
        "\n",
        "predictions = (predict_standart + predict_mobile) / 2\n",
        "\n",
        "def top3(predictions):\n",
        "    \n",
        "    classes = np.load(os.path.join(PATH, 'classes.npy'))\n",
        "    top_3 =  np.argsort(-predictions)[:, 0:3]\n",
        "    top_3_name = np.empty(top_3.shape, dtype=object)\n",
        "    \n",
        "    for row in range(len(top_3)):\n",
        "        for element in range(3):\n",
        "            top_3_name[row,element] = re.sub(' ', '_', classes[top_3[row,element]]) \n",
        "    \n",
        "    return(top_3_name)\n",
        "  \n",
        "top = top3(predictions)\n",
        "\n",
        "preds_df = pd.DataFrame({'first': top[:,0], 'second': top[:,1], 'third': top[:,2]})\n",
        "preds_df['words'] = preds_df['first'] + \" \" + preds_df['second'] + \" \" + preds_df['third']\n",
        "\n",
        "sub = pd.read_csv('/content/drive/My Drive/colab/dataset/' + 'sample_submission.csv', index_col=['key_id'])\n",
        "sub['word'] = preds_df.words.values\n",
        "sub.to_csv('/content/drive/My Drive/colab/dataset_32x32/1class_per_label_proto_Mst.csv')\n",
        "sub.head()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-6b920a863abe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x__test.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMobileNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quick_doodle_M.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m340\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredict_mobile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/colab/dataset_50/50_1/x__test.npy'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "m6b3DhMnQj38",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "59ba7d05-4302-4c0b-cbaf-83ed481b3664"
      },
      "cell_type": "code",
      "source": [
        "def top3(predictions):\n",
        "    \n",
        "    classes = np.load(os.path.join(PATH, 'classes.npy'))\n",
        "    top_3 =  np.argsort(-predictions)[:, 0:3]\n",
        "    top_3_name = np.empty(top_3.shape, dtype=object)\n",
        "    \n",
        "    for row in range(len(top_3)):\n",
        "        for element in range(3):\n",
        "            top_3_name[row,element] = re.sub(' ', '_', classes[top_3[row,element]]) \n",
        "    \n",
        "    return(top_3_name)\n",
        "\n",
        "x_test = np.load(os.path.join(PATH, 'x__test.npy'))\n",
        "\n",
        "predictions = model_import(x_test)\n",
        "\n",
        "top = top3(predictions)\n",
        "\n",
        "preds_df = pd.DataFrame({'first': top[:,0], 'second': top[:,1], 'third': top[:,2]})\n",
        "preds_df['words'] = preds_df['first'] + \" \" + preds_df['second'] + \" \" + preds_df['third']\n",
        "\n",
        "sub = pd.read_csv('/content/drive/My Drive/colab/dataset/' + 'sample_submission.csv', index_col=['key_id'])\n",
        "sub['word'] = preds_df.words.values\n",
        "sub.to_csv('/content/drive/My Drive/colab/dataset_32x32/1class_per_label_proto_Mst.csv')\n",
        "sub.head()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "112199/112199 [==============================] - 16s 143us/step\n",
            "112199/112199 [==============================] - 17s 154us/step\n",
            "112199/112199 [==============================] - 17s 154us/step\n",
            "112199/112199 [==============================] - 17s 154us/step\n",
            "112199/112199 [==============================] - 17s 153us/step\n",
            "112199/112199 [==============================] - 17s 154us/step\n",
            "112199/112199 [==============================] - 17s 154us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>key_id</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9000003627287624</th>\n",
              "      <td>radio stereo train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9000010688666847</th>\n",
              "      <td>hockey_puck sandwich steak</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9000023642890129</th>\n",
              "      <td>The_Great_Wall_of_China castle squiggle</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9000038588854897</th>\n",
              "      <td>mountain triangle pliers</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9000052667981386</th>\n",
              "      <td>campfire fireplace kangaroo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     word\n",
              "key_id                                                   \n",
              "9000003627287624                       radio stereo train\n",
              "9000010688666847               hockey_puck sandwich steak\n",
              "9000023642890129  The_Great_Wall_of_China castle squiggle\n",
              "9000038588854897                 mountain triangle pliers\n",
              "9000052667981386              campfire fireplace kangaroo"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "metadata": {
        "id": "_Mjuu_lklS9R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "ec14bac2-1a84-4813-c280-dd2bd0ff8780"
      },
      "cell_type": "code",
      "source": [
        "(predictions[0,150:200])"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.17698126e-01, 1.03414319e-02, 3.47257010e-03, 9.38992202e-03,\n",
              "       1.05952518e-03, 5.21875045e-04, 1.05166237e-03, 4.47650766e-03,\n",
              "       1.22260640e-03, 2.09903880e-03, 1.21325754e-01, 1.18842057e-03,\n",
              "       2.36647879e-03, 1.30987708e-02, 4.90916878e-01, 5.60439366e-04,\n",
              "       2.37089372e-03, 9.04475222e-04, 2.46729632e-03, 7.09617627e-04,\n",
              "       7.31880544e-04, 1.37150288e-03, 6.79366349e-04, 2.37184041e-03,\n",
              "       9.51877664e-05, 4.99094371e-03, 3.27919573e-02, 8.15031026e-03,\n",
              "       2.21034344e-02, 2.80362163e-02, 6.35030621e-04, 1.07308291e-03,\n",
              "       3.56790088e-02, 2.94828322e-04, 2.31127240e-04, 1.19931647e-03,\n",
              "       2.21713725e-02, 1.98587004e-04, 1.84167305e-03, 4.55079228e-03,\n",
              "       8.74905661e-03, 5.70928957e-03, 1.64937240e-03, 1.90193078e-03,\n",
              "       3.55405034e-03, 5.11326408e-03, 7.24898302e-04, 1.39797824e-02,\n",
              "       4.43272991e-04, 1.73526630e-03], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "metadata": {
        "id": "IWUF_wkzXSU-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "98985190-0bca-4298-850f-1d04a2c0fdb6"
      },
      "cell_type": "code",
      "source": [
        "x_test = np.load('/content/drive/My Drive/colab/dataset_50/.npy')\n",
        "json_file = open('/content/drive/My Drive/colab/dataset_50/50_3/quick_doodle.json', \"r\")\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "# Создаем модель на основе загруженных данных\n",
        "loaded_model_2 = model_from_json(loaded_model_json)\n",
        "# Загружаем веса в модель\n",
        "loaded_model_2.load_weights('/content/drive/My Drive/colab/dataset_50/50_3/quick_doodle.h5')\n",
        "  \n",
        "pred = loaded_model_2.predict(x_test, batch_size=128, verbose=1)\n",
        "  \n",
        "pred[:,:]"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000/50000 [==============================] - 10s 192us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 0.0000000e+00],\n",
              "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 0.0000000e+00],\n",
              "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 0.0000000e+00],\n",
              "       ...,\n",
              "       [0.0000000e+00, 1.6582736e-31, 0.0000000e+00, ..., 3.4264788e-19,\n",
              "        0.0000000e+00, 0.0000000e+00],\n",
              "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 0.0000000e+00],\n",
              "       [0.0000000e+00, 0.0000000e+00, 0.0000000e+00, ..., 0.0000000e+00,\n",
              "        0.0000000e+00, 0.0000000e+00]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "metadata": {
        "id": "66o-EgW7XSTW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5236
        },
        "outputId": "31a8d217-efdd-4fbe-df7c-986f6f72baeb"
      },
      "cell_type": "code",
      "source": [
        "print(x_test)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [ 92]\n",
            "   [192]\n",
            "   [208]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [119]\n",
            "   [  8]\n",
            "   [192]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [ 28]\n",
            "   [187]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]]\n",
            "\n",
            "\n",
            " [[[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[177]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[199]\n",
            "   [  1]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[143]\n",
            "   [121]\n",
            "   [ 96]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]]\n",
            "\n",
            "\n",
            " [[[115]\n",
            "   [160]\n",
            "   [128]\n",
            "   ...\n",
            "   [122]\n",
            "   [  7]\n",
            "   [  0]]\n",
            "\n",
            "  [[127]\n",
            "   [ 65]\n",
            "   [  0]\n",
            "   ...\n",
            "   [146]\n",
            "   [ 66]\n",
            "   [  0]]\n",
            "\n",
            "  [[115]\n",
            "   [ 77]\n",
            "   [  0]\n",
            "   ...\n",
            "   [192]\n",
            "   [ 20]\n",
            "   [  0]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[177]\n",
            "   [ 66]\n",
            "   [ 10]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[ 50]\n",
            "   [110]\n",
            "   [166]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]]\n",
            "\n",
            "\n",
            " [[[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [ 41]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [ 89]\n",
            "   ...\n",
            "   [ 38]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]]\n",
            "\n",
            "\n",
            " [[[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]\n",
            "\n",
            "  [[  0]\n",
            "   [  0]\n",
            "   [  0]\n",
            "   ...\n",
            "   [  0]\n",
            "   [  0]\n",
            "   [  0]]]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}